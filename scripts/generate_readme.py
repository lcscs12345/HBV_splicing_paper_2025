import nbformat
import os
import re
from typing import Union
import logging
import yaml
from pathlib import Path
from collections import Counter


logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')


# description: Functions to generate README files automatically.


# --- Helper Functions ---

def get_repo_root():
    """Finds the repository root by looking for a .git directory."""
    current_dir = Path.cwd()
    while current_dir != current_dir.parent:
        if (current_dir / ".git").is_dir():
            return current_dir
        current_dir = current_dir.parent
    logging.error("Could not find repository root. Please run from inside a Git repository.")
    return None


def infer_description_from_filename(filename):
    """Generate a heuristic description based on filename keywords, prioritizing filename content over extension."""
    name = filename.lower()
    
    if any(ext in name for ext in ['.png', '.svg', '.jpg', '.jpeg', '.pdf', '.tiff']):
        return "Generated plot or figure output."
    elif "mgen" in name:
        return "Supplementary file from our previous study"
    
    # Keyword-based prioritization
    if "cosi_long" in name:
        return "Long-format coSI scores for HBV splice donor and acceptor sites."
    elif "cosi.pkl" in name:
        return "Wide-format coSI scores for HBV splice donor and acceptor pairs."
    elif "percent" in name and "csv" in name:
        return "Final proportions of spliced HBV RNAs used in results."
    elif "cosi" in name and "csv" in name:
        return "Final coSI scores for HBV splice sites used in results."
    elif "crosstab_donors_perc" in name or "crosstab_acceptors_perc" in name:
        return "Cluster-level percentage of true splice site labels (donor/acceptor)."
    elif "crosstab" in name:
        return "Cluster-level splice site usage summary."
    elif "stats" in name:
        return "Splice site statistics across clusters."
    elif "exonicss" in name:
        return "Mapped coSI scores to exonic splice sites."
    elif "sj.out.tab" in name:
        return "STAR splice junction output."
    elif "zeta" in name:
        return "coSI scores for splice sites."
    elif "ipsa" in name:
        return "IPSA pipeline output."
    elif "ssc" in name or "ssj" in name:
        return "Splice site coSI scores from sjcount."
    elif "splicebert" in name:
        return "SpliceBERT model outputs or embeddings."
    elif "consensus" in name:
        if "donor" in name or "acceptor" in name:
            return "Consensus splice donor/acceptor site coordinates."
        elif "fa" in name or "fasta" in name:
            return "Consensus HBV pgRNA sequences."
        else:
            return "Consensus splice site coordinates."
    elif "donor" in name or "acceptor" in name:
        return "Splice donor/acceptor site coordinates."
    elif "pred" in name:
        return "Gene prediction or annotation file."
    elif "log" in name:
        return "Log file from model execution or processing."

    # Extension-based fallback
    if re.search(r'\.gtf$', name):
        return "Transcript annotation file in GTF format."
    elif re.search(r'\.gfx$', name):
        return "Transcript annotation file generated by IPSA pipeline (GFX format)."
    elif re.search(r'\.bed$', name):
        return "Genomic features in BED format."
    elif re.search(r'\.fa$|\.fasta$', name):
        return "FASTA file containing nucleotide sequences."
    elif re.search(r'\.vcf$|\.bcf$', name):
        return "Variant call file (VCF/BCF format)."
    elif re.search(r'\.txt$', name):
        return "Tab-delimited text file with processed results."
    elif re.search(r'\.json$', name):
        return "JSON file containing structured data."
    elif re.search(r'\.csv$', name):
        return "Comma-separated values file with tabular data."
    elif re.search(r'\.pkl$', name):
        return "Serialized Python object (pickle format)."
    elif re.search(r'\.map$', name):
        return "Coordinate mapping file in MAFFT mapout format."
    
    return "Processed file generated via Bash or Python command in notebook."


def read_notebook_metadata(notebook_path):
    """
    Reads the first markdown cell of a notebook for a title and description.
    Assumes the title is the first H1 heading and the description follows.
    """
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook = nbformat.read(f, as_version=4)

        title = None
        description = []
        for cell in notebook.cells:
            if cell.cell_type == 'markdown':
                lines = cell.source.splitlines()
                for line in lines:
                    if line.strip().startswith('# ') and not title:
                        title = line.strip('# ').strip()
                        continue
                    if title: # Collect lines after title until empty or new heading
                        if line.strip() and not line.strip().startswith('#'):
                            description.append(line.strip())
                        else:
                            break # Stop at first empty line or new heading after title
                if title:
                    break # Stop after finding title and its description

        return title, '\n'.join(description).strip() if description else ""

    except Exception as e:
        logging.warning(f"Could not read metadata from {notebook_path}: {e}")
        return None, None

    

# --- Helper function for robust path validation ---
def _validate_path_string(path_str: str, require_directory_path: bool) -> bool:
    """
    Applies a series of robust filters to determine if a string is a valid file path.
    """
    # Strip quotes before validation if they are present
    path = path_str.strip('\'"')

    # 1. Basic checks
    if not path or path.strip() == '': # Empty or whitespace-only
        logging.debug(f"Filtered out (empty/whitespace): '{path_str}'")
        return False
    if path.startswith(('http://', 'https://', 'ftp://')): # Explicitly exclude URLs
        logging.debug(f"Filtered out (URL): '{path_str}'")
        return False
    if '.ipynb_checkpoints' in path: # Exclude checkpoint dirs
        logging.debug(f"Filtered out (checkpoint): '{path_str}'")
        return False

    # 2. Apply require_directory_path filter
    if require_directory_path:
        path_obj = Path(path)
        # Check if the path has a parent directory component other than '.' or ''
        if str(path_obj.parent) == '.' or str(path_obj.parent) == '':
            logging.debug(f"Filtered out (no directory, required): '{path_str}'")
            return False

    # 3. Robust regex-based filters from your previous version
    if (
        path.isdigit() or                                      # Path is just a number (e.g., '123')
        re.match(r'^[{}][\w,]+[{}]$', path) or                 # Looks like a dictionary/set literal (e.g., '{a,b}')
        re.match(r'^-[\w_]+$', path) or                        # Command-line flags (e.g., '-f', '--help')
        re.match(r'^[=\w-]+$', path) or                        # Simple arguments (e.g., '=1.5', 'True', 'False', 'my-arg')
        re.search(r'[\\;,$&*`]', path) or                      # Contains problematic shell characters (typically not in valid paths)
        (re.search(r'\s', path) and not (path.startswith('"') and path.endswith('"')) and not (path.startswith("'") and path.endswith("'"))) or # Spaces in unquoted paths
        not re.search(r'\.([a-zA-Z0-9]{2,5})$', path) or       # MUST have a file extension of 2-5 alphanumeric chars
        re.search(r'\.\d+$', path)                             # Explicitly excludes extensions that are just numbers (e.g., 'file.1', 'file.2')
    ):
        logging.debug(f"Filtered out (general pattern match): '{path_str}'")
        return False
        
    return True



# --- Updated extract_file_usages function ---
def extract_file_usages(notebook_path: Path, project_root: Path, require_directory_path: bool = True) -> Union[tuple[dict[str, str], dict[str, str]], tuple[dict, dict]]:
    """
    Extracts file paths (inputs and outputs) from a Jupyter notebook's code cells.
    Paths are resolved relative to the notebook's location and then normalized
    to be relative to the project_root.
    """
    inputs = {}
    outputs = {}
    
    # Combined regex patterns for both Python and Bash, capturing broader strings
    # Filter out non-paths using _validate_path_string
    file_extraction_patterns = {
        'input': [
            # Python read functions (e.g., pd.read_*, np.load, open, Image.open, custom reads)
            # This pattern is broader, capturing any string in quotes,
            # then validation will filter it.
            r"(?:pd\.|np\.|io\.|Image\.)?(?:read_csv|read_excel|read_pickle|read_json|read_fwf|read_table|read_parquet|load|open|imread)\(\s*['\"](?P<file_path>[^'\"]+)['\"]\s*(?:,|\))",
            r"(?:plt\.imread)\(\s*['\"](?P<file_path>[^'\"]+)['\"]\s*(?:,|\))",
            r"(?:readRDS|load)\(\s*['\"](?P<file_path>[^'\"]+)['\"]\s*(?:,|\))" # R specific (if notebooks run R kernels)
        ],
        'output': [
            # Python write functions (e.g., to_*, np.save, fig.savefig, plt.savefig, open, io.open, custom writes)
            # Broader pattern
            r"(?:to_csv|to_excel|to_pickle|to_json|to_parquet|np\.save|np\.savez|fig\.savefig|plt\.savefig|io\.open|open|write_csv|write_excel|save_data)\(\s*['\"](?P<file_path>[^'\"]+)['\"]\s*(?:,|\))",
            r"(?:saveRDS|save)\(\s*['\"](?P<file_path>[^'\"]+)['\"]\s*(?:,|\))", # R specific
            # Bash outputs/moves (redirection, mv, cp) - RE-INTEGRATED
            r'(?:>\s*|>>\s*|2>\s*|2>&1\s*>\s*)(?P<file_path>[^\s|&;]+)', # Bash redirection (capture anything until space/pipe/ampersand/semicolon)
            r'\b(?:mv|cp)\s+[^\s]+\s+(?P<file_path>[^\s|&;]+)' # Bash mv/cp destination (capture the last non-space arg)
        ]
    }

    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook_content = nbformat.read(f, as_version=4)
    except Exception as e:
        logging.error(f"Error reading notebook {notebook_path}: {e}")
        return {}, {}

    notebook_dir = notebook_path.parent # Get the directory containing the notebook

    for cell in notebook_content.cells:
        if cell.cell_type == 'code':
            # Flatten code for easier regex matching across lines
            code = cell.source.replace('\\\n', ' ').replace('\n', ' ')

            for file_type, patterns in file_extraction_patterns.items():
                for pattern in patterns:
                    for match in re.finditer(pattern, code):
                        raw_file_path_str = match.group('file_path')

                        # --- Apply validation and filtering ---
                        if not _validate_path_string(raw_file_path_str, require_directory_path):
                            continue # Skip if validation fails

                        # --- Path Resolution (as implemented previously) ---
                        # Ensure quotes are stripped from the path before creating a Path object
                        path_to_resolve = raw_file_path_str.strip('\'"')
                        
                        # Resolve the path relative to the notebook's directory to get an absolute path
                        absolute_file_path = (notebook_dir / Path(path_to_resolve)).resolve()

                        # Make the absolute path relative to the project_root
                        try:
                            project_root_relative_path = absolute_file_path.relative_to(project_root)
                        except ValueError:
                            # This means the file is outside the project_root.
                            # Use the absolute path as fallback, or handle as an error.
                            logging.warning(f"File '{absolute_file_path}' for notebook '{notebook_path}' is outside project root '{project_root}'. Using absolute path.")
                            project_root_relative_path = absolute_file_path # Fallback to absolute if not relative to root

                        # Convert to string for dictionary keys
                        final_path_str = str(project_root_relative_path)
                        
                        # Infer description using the basename of the *final* path
                        description = infer_description_from_filename(os.path.basename(final_path_str))
                        
                        if file_type == 'input':
                            inputs[final_path_str] = description
                        else:
                            outputs[final_path_str] = description

    return inputs, outputs



# --- Helper functions for summarization and file writing ---
def _count_file_descriptions(file_dict):
    """Counts files by their inferred description."""
    descriptions_counter = Counter()
    for desc in file_dict.values():
        descriptions_counter[desc] += 1
    return descriptions_counter


def _format_file_summary(file_counts_dict, file_type_label, link_to_comprehensive_readme=None):
    """
    Formats a human-readable summary string for numerous files.
    Optionally includes a link to the comprehensive Project Files README.
    """
    if not file_counts_dict:
        return ""

    parts = []
    total_files = sum(file_counts_dict.values())

    if len(file_counts_dict) == 1:
        desc, count = list(file_counts_dict.items())[0]
        parts.append(f"{count} '{desc}' files")
    else:
        sorted_counts = sorted(file_counts_dict.items(), key=lambda item: item[1], reverse=True)
        top_n_categories = 3 # Number of top categories to explicitly list
        
        for i, (desc, count) in enumerate(sorted_counts):
            if i < top_n_categories:
                parts.append(f"{count} '{desc}' files")
            else:
                remaining_count = sum(c for _, c in sorted_counts[i:])
                parts.append(f"{remaining_count} other files")
                break

    summary_str = f"This notebook {file_type_label} {', '.join(parts)}."
    
    if link_to_comprehensive_readme:
         summary_str += f" For a comprehensive list, see the [Project Files README]({link_to_comprehensive_readme})."
    return summary_str



def write_processed_files_readme(all_files_dict, project_root, processed_data_dir_rel, readme_name):
    readme_content = []
    readme_content.append(f"# Comprehensive List of Detected Project Files\n\n")
    readme_content.append("This file lists all data files detected as inputs to or outputs from the Jupyter notebooks in this repository.\n\n")
    readme_content.append("## Detected Files\n")

    # Get the full path to the directory where PROJECT_FILES.md will be generated
    # This is the base for calculating relative paths for links
    processed_files_readme_dir = project_root / processed_data_dir_rel
    # Ensure this directory exists when the file is written
    processed_files_readme_dir.mkdir(parents=True, exist_ok=True)


    if not all_files_dict:
        readme_content.append("No project files detected by notebooks.\n")
    else:
        # Sort files for consistent output
        sorted_files = sorted(all_files_dict.items())

        for file_path_relative_to_root, description in sorted_files:
            # Construct the absolute path to the file
            absolute_file_path = project_root / file_path_relative_to_root

            # Calculate the path from the location of PROJECT_FILES.md back to the actual file
            # os.path.relpath is excellent for this, handling '..' correctly.
            link_target = os.path.relpath(absolute_file_path, processed_files_readme_dir)

            readme_content.append(f"- [`{file_path_relative_to_root}`]({link_target}): {description}\n")

    # Write the README file
    output_path = processed_files_readme_dir / readme_name
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(readme_content)
        logging.info(f"Generated comprehensive file list at {output_path}")
    except Exception as e:
        logging.error(f"Error writing comprehensive file list to {output_path}: {e}")
        
        

def extract_script_description(script_path: Path, max_lines_to_check: int = 20) -> Union[str, None]:
    """
    Extracts a description from a script file by looking for '# description:' in the first few lines.

    Args:
        script_path (Path): The path to the script file.
        max_lines_to_check (int): The maximum number of lines to read from the start of the file.

    Returns:
        Union[str | None]: The extracted description string, or None if not found or an error occurs.
    """
    try:
        with open(script_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= max_lines_to_check:
                    logging.debug(f"Stopped scanning '{script_path}' after {max_lines_to_check} lines.")
                    break # Stop reading if description isn't in the first N lines

                stripped_line = line.strip()
                # Check for '# description:' case-insensitively
                if stripped_line.lower().startswith('# description:'):
                    # Extract the content after '# description:' and trim whitespace
                    description = stripped_line[len('# description:'):].strip()
                    logging.debug(f"Extracted description from '{script_path}': '{description}'")
                    return description
        logging.debug(f"No '# description:' found in the first {max_lines_to_check} lines of '{script_path}'.")
        return None # No description found in the specified lines
    except Exception as e:
        logging.warning(f"Could not extract description from script '{script_path}': {e}")
        return None




# --- Main README Generation Function ---

def generate_readme_content(config, project_root):
    readme_lines = []
    all_processed_files = {} # Master dictionary to collect all detected files

    # --- Configuration parsing ---
    project_title = config.get('project_title', os.path.basename(project_root)).replace('_', ' ')#.title()
    notebooks_base_dir = Path(project_root) / config.get('notebooks_dir', 'notebooks')
    processed_data_dir_rel = config.get('processed_data_dir', 'data/processed_files')
    file_listing_threshold = config.get('file_listing_threshold', 10)
    
    generate_comp_readme = config.get('generate_processed_files_readme', False)
    comp_readme_name = config.get('processed_files_readme_name', 'PROJECT_FILES.md')
    link_to_comp_readme = f"{processed_data_dir_rel}/{comp_readme_name}" if generate_comp_readme else None


    # --- Main README sections ---
    readme_lines.append(f"# {project_title}\n\n")
    if 'project_description' in config:
        readme_lines.append(config['project_description'] + "\n\n")
    if 'setup_instructions' in config:
        readme_lines.append("## Setup and Installation\n")
        readme_lines.append(config['setup_instructions'] + "\n\n")

    # --- Data Description (Updated to link to comprehensive list) ---
    readme_lines.append("## Data Description\n")
    if 'processed_data_description' in config and config['processed_data_description'].strip():
        readme_lines.append(config['processed_data_description'] + "\n")
    else:
        readme_lines.append("This project processes and generates various data files. Details on specific inputs and outputs are provided within each notebook's description below.\n")

    if generate_comp_readme:
        readme_lines.append(f"For a comprehensive list and descriptions of all automatically detected project files, please refer to [`{link_to_comp_readme}`]({link_to_comp_readme}).\n")
    
    if 'file_descriptions' in config and config['file_descriptions']:
        readme_lines.append("### Key Project Data Files (Manually Curated)\n")
        for relative_path, description in config['file_descriptions'].items():
            readme_lines.append(f"- `{relative_path}`: {description}\n")
        readme_lines.append("\n")
    readme_lines.append("\n")


    # --- Notebooks Section (Enhanced with file summarization) ---
    readme_lines.append("## Notebooks\n")
    if notebooks_base_dir.is_dir():
        all_ipynb_paths = sorted(notebooks_base_dir.rglob('[0-9]*.ipynb'))
        
        notebook_paths = []
        for p in all_ipynb_paths:
            # Convert to string to easily check for the directory name in the path
            if '.ipynb_checkpoints' not in str(p):
                notebook_paths.append(p)
                
        if not notebook_paths:
            readme_lines.append(f"No Jupyter notebooks found in `{notebooks_base_dir}`.\n")
        else:
            for notebook_path in notebook_paths:
                relative_nb_path = notebook_path.relative_to(project_root)
                nb_title, overview_content = read_notebook_metadata(notebook_path)

                if not nb_title:
                    nb_title = notebook_path.stem.replace('_', ' ').title()
                
                github_url_prefix = config.get('github_notebook_url_prefix', '')
                if github_url_prefix:
                    full_nb_url = f"{github_url_prefix}/{relative_nb_path}"
                    readme_lines.append(f"### [{nb_title}]({full_nb_url})\n")
                else:
                    readme_lines.append(f"### {nb_title}\n")

                if overview_content:
                    readme_lines.append(overview_content + "\n")
                else:
                    readme_lines.append(f"*(No detailed overview found in notebook {relative_nb_path})*\n")

                # Extract files for this notebook
                require_dir = config.get('extract_files_require_directory_path', True)
                nb_inputs, nb_outputs = extract_file_usages(notebook_path, project_root, require_directory_path=require_dir)

                # Collect all detected files into the master list
                all_processed_files.update(nb_inputs)
                all_processed_files.update(nb_outputs)

                # Handle Input Files
                if nb_inputs:
                    readme_lines.append("- **Input Files:**\n")
                    if len(nb_inputs) > file_listing_threshold:
                        input_counts = _count_file_descriptions(nb_inputs)
                        summary_text = _format_file_summary(input_counts, "uses", link_to_comp_readme)
                        readme_lines.append(f"  - {summary_text}\n")
                    else:
                        for path, desc in sorted(nb_inputs.items()):
                            readme_lines.append(f"  - `{path}`: {desc}\n")

                # Handle Output Files
                if nb_outputs:
                    readme_lines.append("- **Output Files:**\n")
                    if len(nb_outputs) > file_listing_threshold:
                        output_counts = _count_file_descriptions(nb_outputs)
                        summary_text = _format_file_summary(output_counts, "generates", link_to_comp_readme)
                        readme_lines.append(f"  - {summary_text}\n")
                    else:
                        for path, desc in sorted(nb_outputs.items()):
                            readme_lines.append(f"  - `{path}`: {desc}\n")
                
                readme_lines.append("\n")
    # This is the correct location for the 'else' block for the 'notebooks' section
    else:
        readme_lines.append(f"Notebooks directory `{notebooks_base_dir}` not found.\n\n")

    # --- Scripts Section ---
    scripts_base_dir = project_root / config.get('scripts_dir', 'scripts')
    script_descriptions_manual = config.get('script_descriptions', {}) # Rename for clarity

    if scripts_base_dir.is_dir():
        readme_lines.append("## Scripts\n")
        
        script_extensions = ['*.py', '*.sh', '*.R', '*.r', '*.pl'] # Define relevant script extensions
        all_potential_script_paths = [] # Temporarily hold all paths, including checkpoints
        for ext in script_extensions:
            all_potential_script_paths.extend(sorted(scripts_base_dir.rglob(ext)))

        all_script_paths = []
        for p in all_potential_script_paths:
            if '.ipynb_checkpoints' not in str(p):
                all_script_paths.append(p)
        
        if not all_script_paths:
            readme_lines.append(f"No scripts found in `{scripts_base_dir}`.\n")
        else:
            for script_path in all_script_paths:
                relative_script_path_str = str(script_path.relative_to(project_root))
                
                # 1. Try manual override from readme_config.yml
                description = script_descriptions_manual.get(relative_script_path_str)

                # 2. If no manual description, try extracting from the script file
                if not description:
                    extracted_desc = extract_script_description(script_path)
                    if extracted_desc:
                        description = extracted_desc
                
                # 3. Use the found description or a fallback message
                if description:
                    github_prefix = config.get('github_notebook_url_prefix', '')
                    script_link = f"[{relative_script_path_str}]({github_prefix}/{relative_script_path_str})" if github_prefix else f"`{relative_script_path_str}`"
                    readme_lines.append(f"- {script_link}: {description}\n")
                else:
                    readme_lines.append(f"- `{relative_script_path_str}`: *No description found.*\n") # Updated fallback message
            readme_lines.append("\n")
    else:
        readme_lines.append(f"Scripts directory `{scripts_base_dir}` not found.\n\n")
    
    # --- Other sections ---
    if 'usage_instructions' in config:
        readme_lines.append("## Usage\n")
        readme_lines.append(config['usage_instructions'] + "\n\n")
    if 'license' in config:
        readme_lines.append("## License\n")
        readme_lines.append(config['license'] + "\n\n")
    if 'acknowledgements' in config:
        readme_lines.append("## Acknowledgements\n")
        readme_lines.append(config['acknowledgements'] + "\n\n")

    # Write the comprehensive project files README
    if generate_comp_readme:
        write_processed_files_readme(all_processed_files, project_root, processed_data_dir_rel, comp_readme_name)

    return "".join(readme_lines)


def main():
    repo_root = get_repo_root()
    if not repo_root:
        return

    config_path = repo_root / "readme_config.yml"
    if not config_path.is_file():
        logging.error(f"Configuration file not found at {config_path}. Please create one.")
        return

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except yaml.YAMLError as e:
        logging.error(f"Error reading readme_config.yml: {e}")
        return

    readme_content = generate_readme_content(config, repo_root)

    readme_path = repo_root / "README.md"
    try:
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        logging.info(f"README.md successfully generated at {readme_path}")
    except Exception as e:
        logging.error(f"Failed to write README.md: {e}")

if __name__ == "__main__":
    main()